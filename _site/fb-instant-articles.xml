<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://localhost:4000</link>
    <description>
      On this website, I write my thoughts, tasks and everything that doesn't belong anywhere else.
    </description>
    
        
            <item>
                <title>The Great AI Arbitrage</title>
                <link>http://localhost:4000/technology/data/generative-ai/llm/2025/09/17/great-ai-arbitrage/</link>
                <content:encoded>
                    <![CDATA[
                    <p>The recent <a href="https://nanda.media.mit.edu/ai_report_2025.pdf">MIT NANDA study</a> (State of AI in Business 2025) on the business adoption of generative AI has been eye-opening for many, including especially those who believed that the ever-improving model quality from leading labs meant that they were ripe for disrupting cognitive functions in age-old industrial processes. The study showed, amongst other things, that this was by far not the case, and that almost 95% of the generative AI use-case implementations didn‚Äôt end up in sustained and profitable usage in companies.</p>

<p>While there‚Äôs a lot to unpack and opine subjectively on the study (especially considering that the sample used in the study is only 52 companies), I‚Äôm going to use this single finding that I find the most interesting, as a jumping-off point to this series of articles</p>

<blockquote>
  <p><em>‚ÄúWhile only 40% of companies say they purchased an official LLM subscription, workers from over 90% of the companies we surveyed reported regular use of personal AI tools for work tasks.‚Äù State of AI in Business 2025, MIT NANDA</em></p>

</blockquote>

<p>That should not be surprising to any of us who are outside research settings. Anyone who has worked in a company larger than 20 employees has, at some point, run into this scenario - there‚Äôs a new LLM or coding agent in town, you‚Äôd like to put a credit card on file to start and feel around the vibe of the tool before making a decision to switch, but getting your request approved by management is so cumbersome that you end up using your personal CC for trying out. You might defer the official request until later, if you even expect the request to eventually be cleared by the CISO - who may refuse the eventual request due to data provenance reasons.</p>

<p>But what <em>is</em> remarkable about this finding is the stark contrast it puts against the <em>official</em> initiatives that enterprises undertake with generative AI in general, and LLMs in particular. In addition to the 40% figure of companies purchasing official licensing, there‚Äôs of course the famous <em>5%</em> figure that refers to companies that report profitable and sustained production implementations with the technology.</p>

<p>In this article, we will use the MIT study itself, but with a quantitative breakdown, to answer the question</p>

<aside>
üí°

Why can‚Äôt organizations capture the value from generative AI that individual employees so effortlessly access?

</aside>

<p>In particular, we will examine three critical aspects related to enterprise generative AI adoption -</p>

<ol>
  <li>Why the inference economics actually favour model providers at the current pricing already?</li>
  <li>Where the real adoption of generative AI is happening?</li>
  <li>What capability gaps exist that explain the divide between individual and enterprise-level AI adoption?</li>
</ol>

<p>In the follow-up articles in the series, we will then go on to deduce how enterprises can assess whether their planned generative AI POCs are likely to succeed or fail, based on the use-case categorization of the MIT Nanda study.</p>

<h2 id="the-silicon-floor">The Silicon Floor</h2>

<p>When we distinguish between ‚Äúconsumer‚Äù and ‚Äúenterprise‚Äù use-cases of LLMs, the temptation is to compare ChatGPT-like applications to the early days of Uber and Zomato - that the main objective <em>must</em> be to capture market segments at all (VC‚Äôs) costs, avoid commoditization and only then aim for profitability. However, one only needs to work through the economics of LLM APIs to see that what we‚Äôre witnessing today is anything but comparable to the gig-economy days. In this section, we will quantitatively show that model providers are, in fact, well on their path to profitability on their consumer products, if not already there.</p>

<p><img src="/images/training_summary.png" alt="The capex view of Llama3‚Äôs training costs. For full analysis can be found [here](/files/llm_training_economics.html). " />
<em>The capex view of Llama3‚Äôs training costs. For full analysis can be found <a href="/files/llm_training_economics.html">here</a>.</em></p>

<p>While training costs for LLMs are substantial - our analysis shows Llama 3 herd of models required between <a href="/files/llm_training_economics.html">$197M-$748M in total development costs</a> when including experimental runs, infrastructure, and electricity costs - these represent one-time expenses. For API providers, the inference costs far outweigh training costs. This has been corroborated directly by several credible sources, such as Yann LeCun who <a href="https://www.businessinsider.com/meta-yann-lecun-ai-scientist-deepseek-markets-reaction-inference-2025-1?utm_source=chatgpt.com">said</a> in context of the erstwhile media frenzy over Deepseek</p>

<blockquote>
  <p><em>‚ÄúThe huge sums of money going into US AI companies were needed primarily for inference, not training AI. Most of the infrastructure cost for AI is for inference: serving AI assistants to billions of people.‚Äù Yann LeCun, Chief AI Scientist at Meta</em></p>

</blockquote>

<p>With this in mind, in the rest of this section, we‚Äôll just go ahead and assume that the cost of inference (runtime serving costs - GPU rental or own-cost amortization, power, cooling, datacenter cost) is the main driver for deciding operating margins for model providers, such as OpenAI and Anthropic, and not training. This may, of course, not hold true for business models other than model-as-an-API, but we‚Äôll come to alternative business models for foundational companies at a later point.</p>

<p><img src="/images/paid-vs-full-infographic-inference.png" alt="OpenAI‚Äôs estimated profit margins on ChatGPT consumer app alone. The costs are for COGS only. For details refer to the [full breakdown](/files/llm_inference_economics_full_plus_api_merged.html). " />
<em>OpenAI‚Äôs estimated profit margins on ChatGPT consumer app alone. The costs are for COGS only. For details refer to the <a href="/files/llm_inference_economics_full_plus_api_merged.html">full breakdown</a>.</em></p>

<p>As for the economics of LLM inference, a definitive answer as to the model providers‚Äô economics is hard to come by. Various sources claim different numbers on total cost of ownership of models and serving infrastructure, and the major sources of contention are</p>

<ol>
  <li>Whether there are enough paid monthly subscribers to cover the inference cost for free users</li>
  <li>Whether there are enough <em>low usage</em> paid subscribers subsidizing the costs for heavy users</li>
  <li>Whether there are enough consumption-based API users (e.g. coding agents) where the costs and earnings are easier to attribute than monthly subscriptions.</li>
</ol>

<p>Our <a href="/files/llm_inference_economics_full_plus_api_merged.html">own calculations</a>, using Deepseek-v3.1 MoE (37B active params) as proxy for SOTA closed models, paint an ambiguous picture about potential revenues and margins for top model providers such as OpenAI and Anthropic. Assuming a <a href="https://sqmagazine.co.uk/openai-vs-anthropic-statistics/">reported</a> ~190‚Äì200M DAU for ChatGPT, we estimate that OpenAI may be spending between 300 to 700M USD per month on inference for their chat app users. Assuming 10% of those DAUs are paid users, they might be earning a monthly revenue between 350 to 400M USD. This indicates that, with tailored optimization strategies per user-segment, OpenAI might either already be profitable on ChatGPT app, or close to getting there.</p>

<p>For API customers, the economics are somewhat <a href="/files/llm_inference_economics_full_plus_api_merged.html">different</a>: developers and enterprise clients paying ~$10 per million input tokens and ~$20 per million output tokens may provide gross margins ranges between -50% to 70%, making the API business a lever for OpenAI and Anthropic‚Äôs to cross-subsidize their consumer application business, if needed.</p>

<p>A detailed breakdown of all our calculations may be seen <a href="/files/llm_inference_economics_full_plus_api_merged.html">here</a>. Additional sources of <em>non-service</em> revenues that we haven‚Äôt included in our analysis are the revenue share that OpenAI and Anthropic might earn on their enterprise inference deals with Azure (Azure OpenAI), AWS (Bedrock) or Google (Vertex AI Partner Models).</p>

<p>Therefore, even assuming considerable deviations in the reported usage data and model details, the economics of GA inference currently favour the market leaders in generative AI. OpenAI, Anthropic, Google Deepmind and such, for all we can observe, have not made any major moves to squeeze their margins yet, which should indicate how different these dynamics are to the gig economy days.</p>

<h2 id="the-shadow-economy-paradox">The ‚ÄúShadow Economy‚Äù Paradox</h2>

<p>As we have seen above, the major foundation model providers can already provide access to generative AI technology profitably to millions of users worldwide. In fact, our calculations reveal that had it not been for the incredible demand for these model APIs, the model providers may never have been able to operate profitably, i.e. the economy of GPU utilization works out for them only due to the scale.</p>

<p>It would be fair to say that a major chunk of the daily active users of these chat-style applications are employees of the very enterprises that report little to no gains in efficiency with generative AI (more on that soon).</p>

<p>What I find remarkable about this figure is that a significant share of employees reporting productivity boost with tools such as ChatGPT are using their private subscriptions to these services, for professional purpose (the ‚Äúshadow economy‚Äù).</p>

<blockquote>
  <p><em>‚ÄúA significant number of workers already use AI tools privately, reporting productivity gains, while their companies‚Äô formal AI initiatives stall.‚Äù State of AI in Business 2025, MIT NANDA</em></p>

</blockquote>

<p>The reason this fact is remarkable is that these employees have wildly different feedback for the task-specific generative AI tooling and solutions that their own companies were developing at the time.</p>

<blockquote>
  <p><em>‚ÄúA significant number of workers already use AI tools privately, reporting productivity gains, while their companies‚Äô formal AI initiatives stall.‚Äù State of AI in Business 2025, MIT NANDA</em></p>

</blockquote>

<p>In summary, the foundation model providers are already profitable off the back of the day-to-day productivity increase of employees of the same companies, whose own custom AI tools largely suck.</p>

<p>So, why don‚Äôt these enterprises just let their employees use company credit cards for Claude, instead of sinking millions into failed custom generative AI tooling?</p>

<h2 id="the-real-adoption-barriers">The Real Adoption Barriers</h2>

<p>It should be clear to the readers that it is not due to a mismatch in the fundamental model economics, training-time or inference-time, that only 5% of the enterprises deploying generative AI pilots end up extracting regular and sustained business value out of them. The MIT study itself points to <em>technological</em> <em>gaps</em> that are, in fact, to blame.</p>

<p>The study assigns an overarching term to this identified gap in LLM-based applications - ‚ÄúThe Learning Gap‚Äù. Basically, the LLMs - and precisely <em>not</em> the chat apps - suffer from the following glaring insufficiencies that make them unsuitable to be used as trustworthy task-specific companions, experts or - in terms that we‚Äôve all come to accept - ‚Äú<em>copilots‚Äù</em>:</p>

<ol>
  <li>The LLMs themselves don‚Äôt learn over time from the users‚Äô feedback.</li>
  <li>As a corollary to the above, the LLMs require too much context to be supplied every time, and there isn‚Äôt a straightforward and <em>effective</em> way for the right context to be dynamically chosen based on the task-at-hand (üëÄ¬†vector search).</li>
  <li>Even if the LLM-based workflows complete tasks without intervention <em>most</em> of the time, they break for edge-cases and don‚Äôt learn from these failures. Since, by definition, these edge cases are more complicated to define and expect, the overall trustworthiness of the LLM-based tooling suffers as a result.</li>
</ol>

<blockquote>
  <p><em>‚ÄúOur purchased AI tool provided rigid summaries with limited customization options. With ChatGPT, I can guide the conversation and iterate until I get exactly what I need. The fundamental quality difference is noticeable, ChatGPT consistently produces better outputs, even though our vendor claims to use the same underlying technology.‚Äù</em> - Corporate lawyer at a mid-sized firm quoted in <em>State of AI in Business in 2025, MIT NANDA</em></p>

</blockquote>

<p>Enterprise employees, being well-versed in current SOTA tools and models, are able to successfully augment their own job functions precisely because they don‚Äôt expect ChatGPT et al. to retain feedback, or improve over time. We could go as far as to suggest that employees <em>don‚Äôt</em> consider AI chatbots to be replacements for <em>any</em> level of expertise in their job functions - not even interns.</p>

<p>This is in stark contrast to ambitious enterprise generative AI pilots, whose stated aim often is to directly augment their workforce. Such pilots then, obviously, lead to total disenchantment since the technology clearly is not able to accumulate knowledge over time, retain relevant context (and, importantly, decide which context needs to be chucked away), and basically train itself on the job like a junior or associate-level employee would.</p>

<p>As it stands, SOTA LLMs don‚Äôt stand a chance at getting promoted in the next appraisal cycle.</p>

<h2 id="the-opportunity">The Opportunity</h2>

<p>So how are enterprises and startups going to respond to these key challenges (profitable TCO not being one of them) that make it difficult for generative AI initiatives to succeed? In the next articles, we will discuss some concrete failure patterns that companies regularly run into such as lost opportunity costs of fine-tuning LLMs, and applying LLMs only for low-leverage job functions. Then we will make direct suggestions based on current research and available enterprise adoption data, as to how one should prioritize use-cases that are well-suited to be augmented with capabilities that current LLMs possess.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/technology/data/generative-ai/llm/2025/09/17/great-ai-arbitrage/</guid>
                <description>
                    
                    Technology, not viability, is the reason for disillusionment with Generative AI
                    
                </description>
                <pubDate>Wed, 17 Sep 2025 09:00:00 +0200</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>The Rift Between Open-source and Commercial Software Is Not Irreconcilable</title>
                <link>http://localhost:4000/technology/software/open-source/product/2022/11/07/open-source-vs-commercial/</link>
                <content:encoded>
                    <![CDATA[
                    <p>The most notable thing that makes the development of commercial software and FOSS different is the culture of being product focused vs. one of sharing.</p>

<p>‚ÄúWhat can we engineer that makes us the most money and sustain?‚Äù vs. ‚ÄúI love this thing I made, I‚Äôm sure there‚Äôs someone out there that finds it cool‚Äù.</p>

<p>I‚Äôm probably being unfair with the ‚Äúmakes money‚Äù part (disclosure: I work on commercial software all day), but you get my drift - feel free to replace it with ‚Äúgets the most users to stay‚Äù or ‚Äúgets the most word out‚Äù etc.</p>

<p>However, all that is not to say that one culture of development is better than the other, but I do think that acknowledging the difference is important for developers and aligned personas to appreciate different motivations better. While the former leads to coherent product experiences, the latter leads to crazy (good) experiments, creative masterworks and worthwhile alternatives to drab experiences.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/technology/software/open-source/product/2022/11/07/open-source-vs-commercial/</guid>
                <description>
                    
                    Can we all please just get along?
                    
                </description>
                <pubDate>Mon, 07 Nov 2022 08:00:00 +0100</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>Data Quality Metrics for Everybody</title>
                <link>http://localhost:4000/data/technology/ai/2022/08/17/data-quality/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Training the first ML model in a business app is usually driven by extensive experimentation, exploration and, what is essentially, making a lot of reasonable assumptions about what a future user‚Äôs characteristics are and how they will behave. At inference time, however, nobody should be surprised when these assumptions are violated by reality left, right and centre, and you‚Äôre left with some pretty tricky scenarios to diagnose and debug. As a starting point for these investigations, which should, ideally, involve feature importances, explanations, and so on, should be a good grip on whether the latest training data, as well as inference-time input meets a basic set of quality measures.</p>

<p>This article won‚Äôt go into any business use-case specific assumptions, obviously, but the following are the bare minimum in terms of data quality metrics that you should put in place, and automate, at the end of your ETL pipelines and inference endpoints.</p>

<ol>
  <li><em>Null values</em>: Whether there are missing values in fields where there shouldn‚Äôt be any.</li>
  <li><em>Casting of values</em>: Whether the input data flows in in a format that your casting operations can‚Äôt handle, e.g. string ‚Äú1.0‚Äù can be cast into floating point, but ‚Äú1 point something‚Äù cannot be.</li>
  <li><em>Fill rates</em>: For sparse feature matrices, an overview of column fill rates is important, especially how it has evolved over the time since your model was last trained.</li>
  <li><em>Timeliness</em>: If models are trained periodically upon availability of new ground-truth data, then a timeliness check is what allows data scientist to track whether <em>all the expected data</em> was updated, and to what degree, at the expected intervals.</li>
  <li><em>Historical consistency</em>: As a final check, historical consistency should give an indication of whether any <em>qualitative</em> aspects of the data have changed over time, e.g. has a numerical field always been delivering integer values or have the sources been recently delivering floating point values instead - denoting certain new assumptions about the data.</li>
</ol>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/data/technology/ai/2022/08/17/data-quality/</guid>
                <description>
                    
                    To solve one of the biggest pains in ML projects, you need to start somewhere - here.
                    
                </description>
                <pubDate>Wed, 17 Aug 2022 09:00:00 +0200</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>The AI of Your ML Products Can Also be an Attack Surface</title>
                <link>http://localhost:4000/technology/ai/machine-learning/security/deep-learning/2022/06/29/security-in-ml/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Even though non-ML fields in computer science are swiftly turning into auxiliary and domain-specific machine learning research fields (e.g. look at the number of papers using or addressing machine learning in this year‚Äôs <a href="https://conf.researchr.org/program/icse-2022/program-icse-2022/Detailed-Table">International Conference for Software Engineering</a>), the core research in ML and deep learning moves so fast, that it‚Äôs impossible these days for researchers from other fields to keep track of all the new developments.
Serious researchers, however, have <a href="https://ieeexplore.ieee.org/abstract/document/9252851">already</a> <a href="https://ieeexplore.ieee.org/abstract/document/8474192">started</a> <a href="https://ieeexplore.ieee.org/abstract/document/8617013">to look</a> into an important, but often overlooked aspect due to the speed of new developments, in machine learning systems - <em>security</em>.</p>

<p>Security, in this article, is probably a catch-all for a lot of related but, theoretically speaking, distinct concepts such as robustness, safety, and adversarial-proof machine learning systems. If we use the <a href="https://arxiv.org/abs/1806.00098">artifact-based approach</a> to software engineering, security for machine learning systems should focus on these main artifacts -</p>

<ol>
  <li>Training data</li>
  <li>Trained models</li>
  <li>Inference endpoints or APIs</li>
</ol>

<p><strong>Training data</strong>, especially when sourced from ELT pipelines and automatically labeled, can be the first source of poisoning if, either meaningful data quality metrics are not set-up and visible (more on this in a later post), or if active-learning is set up where adversarial edge-case inputs are added to the training datasets without correcting the labels first.</p>

<p><strong>Trained models</strong> can further add to the attack surface of an ML-system when it is not updated over the course of business use-case serving, and they <em>drift</em> due to the initial assumptions about the independent variables or problem statements not being valid anymore. Moreover, it can be argued that deep models whose outputs cannot be sufficiently <em>explained</em>, can be hard to debug and are a compliance and security liability, in themselves.</p>

<p><strong>Inference endpoints</strong>, typically in the form of Rest or GraphQL APIs, may, in addition to being vulnerable to common software vulnerabilities, also be susceptible to being abused by, e.g. training proxy models by attackers or inferring on adversarial inputs.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/technology/ai/machine-learning/security/deep-learning/2022/06/29/security-in-ml/</guid>
                <description>
                    
                    Time to think seriously about security in machine/deep learning
                    
                </description>
                <pubDate>Wed, 29 Jun 2022 13:00:00 +0200</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>Who are our Review Meetings for?</title>
                <link>http://localhost:4000/organization/2022/03/22/review-meetings/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Upcoming</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/organization/2022/03/22/review-meetings/</guid>
                <description>
                    
                    An Ode to Pragmatic Scrum
                    
                </description>
                <pubDate>Tue, 22 Mar 2022 08:00:00 +0100</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>Most Data Fabrics are Made Of Lead, not Linen</title>
                <link>http://localhost:4000/data/technology/ai/2022/03/08/data-fabric/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Gartner defines Data Fabric as</p>
<blockquote>
  <p>‚Äúa design concept that serves as an integrated layer (fabric) of data and connecting processes‚Äù</p>

  <p>‚Äì <cite><a href="https://www.gartner.com/smarterwithgartner/data-fabric-architecture-is-key-to-modernizing-data-management-and-integration">Gartner</a></cite></p>
</blockquote>

<p>It even places itself as a sibling to the data virtualization idea - that proposes a ‚Äúlogical‚Äù data layer on top of disparate sources, where data resides in varying formats. 
While most data fabric providers promise the world at the click of a button, it remains the case that most businesses need to use a semantic engine on top of their original sources, in order to run meaningful data analytics. These semantic engines are what ultimately often plug into an org‚Äôs data lake pipelines and feed into the data fabric, that is able to keep the data fresh for quick analytics.</p>

<p>Creating a domain- or org-specific AI-driven data integration strategy from scratch (and it often <em>need to be</em> from scratch) sounds frightening, especially given how prolific the development in vector-search space has been recently. For D&amp;A teams, this often means top-down mandates and project charters talking about digital transformation, breaking of department-silos, machine learning capabilities, question-and-answer capabilities yada, yada, yada.</p>

<p>But I‚Äôm here to tell you - you‚Äôve got this. And the reason I say this is because, unlike what the hype tries to make you believe, most data integration layers are built using domain-specific, time-consuming and <em>heavy</em> transformations to even bring their original sources to a state that they can index it in a Lucene-based text search engine. And while that might not seem impressive if you‚Äôre on data management LinkedIn too much, it is enormous when it comes to solving your organization‚Äôs specific needs and maybe even create new business models.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/data/technology/ai/2022/03/08/data-fabric/</guid>
                <description>
                    
                    Don't let AI-experts scare you out of implementing data integration in your org
                    
                </description>
                <pubDate>Tue, 08 Mar 2022 08:00:00 +0100</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>Why Aren't Data Behemoths Crushing It Harder?</title>
                <link>http://localhost:4000/technology/data/2022/02/22/digitalization-talking-data/</link>
                <content:encoded>
                    <![CDATA[
                    <p>While the ten oldest newspapers still in circulation <a href="https://en.wikipedia.org/wiki/List_of_the_oldest_newspapers">almost exclusively</a> began in Europe, none of these make it to the list of top newspapers by circulation <a href="https://en.wikipedia.org/wiki/List_of_newspapers_by_circulation">today</a>. An analogous comparison can be made for the oldest banks and insurance companies in the world too - when compared by their total assets or loss ratios, respectively. 
For most industry veterans, it is not for a lack of digitalization schemes that they haven‚Äôt been able to conquer modern markets completely, but - and I‚Äôm obviously biting off a lot more than I can chew - what that digitalization entails.</p>

<p>It isn‚Äôt enough that historical data is digitalized and suitably archived - what matters much more is whether the data is <em>activated</em>. 
Activated data is fresh (despite being old), malleable, and <a href="https://saahil.github.io/technology/philosophy/data/2022/02/10/metaphors">vocal</a>.
Activated data knows what it is.</p>

<p>Semantic web is already a magnificent achievement, in terms of categorizing and standardizing data on the internet. Domain knowledge graphs can even further build upon the semantic web idea to <em>activate</em> data, and enable organizations to make decisions and generate value.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/technology/data/2022/02/22/digitalization-talking-data/</guid>
                <description>
                    
                    Digitalization is more than document scans.
                    
                </description>
                <pubDate>Tue, 22 Feb 2022 08:00:00 +0100</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>Metaphors in Technology</title>
                <link>http://localhost:4000/technology/philosophy/data/2022/02/10/metaphors/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Working closely with semantic web technologies every day, a common refrain we hear is ‚ÄúTalk to your data‚Äù. Talk TO your data? Kind of like talking to a wall, then? Talking to your data, much like talking to a wall, undoubtedly reflects a desperate and frustrating attempt that, unless <em>data talks back</em>, is ultimately futile. Hopefully, your data won‚Äôt just talk back, but <em>sing</em>!</p>

<p>For a discipline whose fundamental building tools are (programming) languages, we, software engineers and statisticians, are notoriously bad at putting metaphors to work - if for nothing else, then to tell better stories with and about (not TO) our products.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/technology/philosophy/data/2022/02/10/metaphors/</guid>
                <description>
                    
                    In a rush to be impressive, it is easy to be shallow
                    
                </description>
                <pubDate>Thu, 10 Feb 2022 08:25:00 +0100</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
        
            <item>
                <title>Innovations and Innovations</title>
                <link>http://localhost:4000/technology/2022/02/04/innovation-types/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Here‚Äôs a interesting paraphrased quote about types of innovation, from a book I‚Äôve been reading this week</p>

<blockquote>
  <p>‚ÄúSteam engines increased global productivity by many orders of magnitude‚Ä¶(while)‚Ä¶1970s saw a decline in American national productivity, seemingly reflected in the kind of innovations such as replacing window room air-conditioners with central building air-conditioners.‚Äù</p>

  <p>‚Äì <cite><a href="https://www.goodreads.com/book/show/23316526-the-second-machine-age">The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies</a>, <em>Andrew McAfee and Erik Brynjolfsson</em> </cite></p>
</blockquote>

<p>The quote kept me up a while trying to think of similar examples</p>

<blockquote>
  <p>‚ÄúAn innovation like eBay, not Facebook‚Äù</p>
</blockquote>

<blockquote>
  <p>‚ÄúAn innovation like lamp shades, not Nest‚Äù</p>
</blockquote>

<blockquote>
  <p>‚Ä¶</p>
</blockquote>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/technology/2022/02/04/innovation-types/</guid>
                <description>
                    
                    A commentary lifted from The Second Machine Age
                    
                </description>
                <pubDate>Fri, 04 Feb 2022 09:00:00 +0100</pubDate>
                <author>Saahil Ognawala</author>
            </item>
        
    
  </channel>
</rss>
